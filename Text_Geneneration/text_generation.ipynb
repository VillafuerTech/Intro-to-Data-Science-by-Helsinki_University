{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Data Science 2025\n",
        "\n",
        "# Week 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this week's exercise, we look at prompting and zero- and few-shot task settings. Below is a text generation example from https://github.com/TurkuNLP/intro-to-nlp/blob/master/text_generation_pipeline_example.ipynb demonstrating how to load a text generation pipeline with a pre-trained model and generate text with a given prompt. Your task is to load a similar pre-trained generative model and assess whether the model succeeds at a set of tasks in zero-shot, one-shot, and two-shot settings.\n",
        "\n",
        "**Note: Downloading and running the pre-trained model locally may take some time. Alternatively, you can open and run this notebook on [Google Colab](https://colab.research.google.com/), as assumed in the following example.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIQ1s96UCcJW"
      },
      "source": [
        "## Text generation example\n",
        "\n",
        "This is a brief example of how to run text generation with a causal language model and `pipeline`.\n",
        "\n",
        "Install [transformers](https://huggingface.co/docs/transformers/index) python package. This will be used to load the model and tokenizer and to run generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4fUBJmXHCHw-"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZRNZgRJCt6Q"
      },
      "source": [
        "Import the `AutoTokenizer`, `AutoModelForCausalLM`, and `pipeline` classes. The first two support loading tokenizers and generative models from the [Hugging Face repository](https://huggingface.co/models), and the last wraps a tokenizer and a model for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jwyK005xCFSF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/villafuertech/.venvs/jlab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QJPDe3ZC_sL"
      },
      "source": [
        "Load a generative model and its tokenizer. You can substitute any other generative model name here (e.g. [other TurkuNLP GPT-3 models](https://huggingface.co/models?sort=downloads&search=turkunlp%2Fgpt3)), but note that Colab may have issues running larger models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wqTxn_QaCNjZ"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'TurkuNLP/gpt3-finnish-large'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ADWWb77e1sY"
      },
      "source": [
        "Instantiate a text generation pipeline using the tokenizer and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0IIJzNrEe5qx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=model.device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAohNr1ciwaU"
      },
      "source": [
        "We can now call the pipeline with a text prompt; it will take care of tokenizing, encoding, generation, and decoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jWcOJkiKi5vr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'Terve, miten menee? Tullaan huomenna, jos kaikki menee hyvin.”\\n”Nuku hyvin, kulta.”\\n”Rakastan sinua.”'}]\n"
          ]
        }
      ],
      "source": [
        "output = pipe('Terve, miten menee?', max_new_tokens=25)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNRMsxXOjSo0"
      },
      "source": [
        "Just print the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9Op7MJ6XjahG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terve, miten menee? Tullaan huomenna, jos kaikki menee hyvin.”\n",
            "”Nuku hyvin, kulta.”\n",
            "”Rakastan sinua.”\n"
          ]
        }
      ],
      "source": [
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YROp3hyikXPO"
      },
      "source": [
        "We can also call the pipeline with any arguments that the model `generate` function supports. For details on text generation using `transformers`, see e.g. [this tutorial](https://huggingface.co/blog/how-to-generate).\n",
        "\n",
        "Example with sampling and a high `temperature` parameter to generate more chaotic output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "22QjXE88jkim"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terve, miten menee? Entä millainen olit koulussa nuorena tai opiskelijana?\n",
            "Minulla siis kaikki sujunut ok vaikka pieni pelko hiipinyt rintaan..\n",
            "Millainen historia oli lukioni aikana\n"
          ]
        }
      ],
      "source": [
        "output = pipe(\n",
        "    'Terve, miten menee?',\n",
        "    do_sample=True,\n",
        "    temperature=10.0,\n",
        "    max_new_tokens=25\n",
        ")\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1\n",
        "\n",
        "Your task is to assess whether a generative model succeeds in the following tasks in zero-shot, one-shot, and two-shot settings:\n",
        "\n",
        "- binary sentiment classification (positive / negative)\n",
        "\n",
        "- person name recognition\n",
        "\n",
        "- two-digit addition (e.g. 11 + 22 = 33)\n",
        "\n",
        "For example, for assessing whether a generative model can name capital cities, we could use the following prompts:\n",
        "\n",
        "- zero-shot:\n",
        "\t>\"\"\"\\\n",
        "\t>Identify the capital cities of countries.\n",
        "\t>\n",
        "\t>Question: What is the capital of Finland?\\\n",
        "\t>Answer:\\\n",
        "\t>\"\"\"\n",
        "- one-shot:\n",
        "\t>\"\"\"\\\n",
        "\t>Identify the capital cities of countries.\n",
        "\t>\n",
        "\t>Question: What is the capital of Sweden?\\\n",
        "\t>Answer: Stockholm\n",
        "\t>\n",
        "\t>Question: What is the capital of Finland?\\\n",
        "\t>Answer:\\\n",
        "\t>\"\"\"\n",
        "- two-shot:\n",
        "\t>\"\"\"\\\n",
        "\t>Identify the capital cities of countries.\n",
        "\t>\n",
        "\t>Question: What is the capital of Sweden?\\\n",
        "\t>Answer: Stockholm\n",
        "\t>\n",
        "\t>Question: What is the capital of Denmark?\\\n",
        "\t>Answer: Copenhagen\n",
        "\t>\n",
        "\t>Question: What is the capital of Finland?\\\n",
        "\t>Answer:\\\n",
        "\t>\"\"\"\n",
        "\n",
        "You can do the tasks either in English or Finnish and use a generative model of your choice from the Hugging Face models repository, for example the following models:\n",
        "\n",
        "- English: `gpt2-large`\n",
        "- Finnish: `TurkuNLP/gpt3-finnish-large`\n",
        "\n",
        "You can either come up with your own instructions for the tasks or use the following:\n",
        "\n",
        "- English:\n",
        "\t- binary sentiment classification: \"Do the following texts express a positive or negative sentiment?\"\n",
        "\t- person name recognition: \"List the person names occurring in the following texts.\"\n",
        "\t- two-digit addition: \"This is a first grade math exam.\"\n",
        "- Finnish:\n",
        "\t- binary sentiment classification: \"Ilmaisevatko seuraavat tekstit positiivista vai negatiivista tunnetta?\"\n",
        "\t- person name recognition: \"Listaa seuraavissa teksteissä mainitut henkilönnimet.\"\n",
        "\t- two-digit addition: \"Tämä on ensimmäisen luokan matematiikan koe.\"\n",
        "\n",
        "Come up with at least two test cases for each of the three tasks, and come up with your own one- and two-shot examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task | Setting | Case | Expected | Predicted | OK\n",
            "sentiment | zero-shot | 1 | positive | unknown | False\n",
            "sentiment | zero-shot | 2 | negative | unknown | False\n",
            "name_recognition | zero-shot | 1 | Alice, Bob | Alice and Bob went to the park.\n",
            "Text: Alice and Bob went to the park.\n",
            "Text: Alice and Bob went to the park.\n",
            "Text | True\n",
            "name_recognition | zero-shot | 2 | Barack Obama, Joe Biden | Joe Biden met Barack Obama in Washington.\n",
            "Text: Barack Obama met Joe Biden in Washington.\n",
            "Text: Barack Obama met Joe Biden in Washington.\n",
            "Text | True\n",
            "two_digit_addition | zero-shot | 1 | 33 | 11 | False\n",
            "two_digit_addition | zero-shot | 2 | 65 | 47 | False\n",
            "sentiment | one-shot | 1 | positive | negative | False\n",
            "sentiment | one-shot | 2 | negative | negative | True\n",
            "name_recognition | one-shot | 1 | Alice, Bob | Alice and Bob went to the park.\n",
            "Text: Alice and Bob went to the park.\n",
            "Text: Alice and Bob went to the park.\n",
            "Text | True\n",
            "name_recognition | one-shot | 2 | Barack Obama, Joe Biden | Barack Obama, Joe Biden\n",
            "Text: Barack Obama, Joe Biden\n",
            "Text: Barack Obama, Joe Biden\n",
            "Text: Barack Obama, Joe Biden\n",
            "Text: | True\n",
            "two_digit_addition | one-shot | 1 | 33 | 23 | False\n",
            "two_digit_addition | one-shot | 2 | 65 | 49 | False\n",
            "sentiment | two-shot | 1 | positive | positive | True\n",
            "sentiment | two-shot | 2 | negative | positive | False\n",
            "name_recognition | two-shot | 1 | Alice, Bob | Alice and Bob went to the park.\n",
            "Text: Alice and Bob went to the park.\n",
            "Text: Alice and Bob went to the park.\n",
            "Text | True\n",
            "name_recognition | two-shot | 2 | Barack Obama, Joe Biden | Barack Obama, Joe Biden\n",
            "Text: Barack Obama, Joe Biden\n",
            "Text: Barack Obama, Joe Biden\n",
            "Text: Barack Obama, Joe Biden\n",
            "Text: | True\n",
            "two_digit_addition | two-shot | 1 | 33 | 37 | False\n",
            "two_digit_addition | two-shot | 2 | 65 | 48 | False\n",
            "\n",
            "Accuracy by task/setting:\n",
            "sentiment / zero-shot: 0/2 = 0.0%\n",
            "name_recognition / zero-shot: 2/2 = 100.0%\n",
            "two_digit_addition / zero-shot: 0/2 = 0.0%\n",
            "sentiment / one-shot: 1/2 = 50.0%\n",
            "name_recognition / one-shot: 2/2 = 100.0%\n",
            "two_digit_addition / one-shot: 0/2 = 0.0%\n",
            "sentiment / two-shot: 1/2 = 50.0%\n",
            "name_recognition / two-shot: 2/2 = 100.0%\n",
            "two_digit_addition / two-shot: 0/2 = 0.0%\n",
            "\n",
            "Notes:\n",
            "- GPT-2 is small and not instruction-tuned, so zero-shot can be weak.\n",
            "- One- and two-shot usually help by showing the format.\n",
            "- NER check only verifies that expected names appear in the model's answer.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import re\n",
        "\n",
        "model_name_en = 'gpt2'  \n",
        "tok_en = AutoTokenizer.from_pretrained(model_name_en)\n",
        "mdl_en = AutoModelForCausalLM.from_pretrained(model_name_en)\n",
        "pipe_en = pipeline('text-generation', model=mdl_en, tokenizer=tok_en, device=mdl_en.device)\n",
        "\n",
        "def generate_en(prompt, max_new_tokens=40):\n",
        "    out = pipe_en(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    return out[0]['generated_text']\n",
        "\n",
        "sentiment_instruction = (\n",
        "    \"Say if the text is positive or negative. Answer with 'positive' or 'negative' only.\\n\"\n",
        ")\n",
        "\n",
        "sentiment_one_shot_example = (\n",
        "    \"Text: The day was amazing and I felt great.\\n\"\n",
        "    \"Answer: positive\\n\"\n",
        ")\n",
        "\n",
        "sentiment_two_shot_example = (\n",
        "    \"Text: The food was cold and the service was slow.\\n\"\n",
        "    \"Answer: negative\\n\"\n",
        ")\n",
        "\n",
        "sentiment_tests = [\n",
        "    {\"text\": \"I love this new laptop, the battery lasts long and the screen is beautiful.\", \"expected\": \"positive\"},\n",
        "    {\"text\": \"The app keeps crashing and it wastes my time. I hate it.\", \"expected\": \"negative\"},\n",
        "]\n",
        "\n",
        "def make_sentiment_prompt(setting, text):\n",
        "    prompt = sentiment_instruction\n",
        "    if setting == 'one-shot':\n",
        "        prompt += sentiment_one_shot_example\n",
        "    if setting == 'two-shot':\n",
        "        prompt += sentiment_one_shot_example + sentiment_two_shot_example\n",
        "    prompt += f\"Text: {text}\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "def check_sentiment(answer_text, expected_label):\n",
        "    ans = answer_text.split('Answer:')[-1].strip().lower()\n",
        "    if 'positive' in ans:\n",
        "        pred = 'positive'\n",
        "    elif 'negative' in ans:\n",
        "        pred = 'negative'\n",
        "    else:\n",
        "        pred = 'unknown'\n",
        "    return pred == expected_label, pred\n",
        "\n",
        "ner_instruction = (\n",
        "    \"List the person names in the text. Return only the names separated by commas.\\n\"\n",
        ")\n",
        "ner_one_shot_example = (\n",
        "    \"Text: Alice met Bob at the library.\\n\"\n",
        "    \"Answer: Alice, Bob\\n\"\n",
        ")\n",
        "ner_two_shot_example = (\n",
        "    \"Text: John and Mary studied together.\\n\"\n",
        "    \"Answer: John, Mary\\n\"\n",
        ")\n",
        "ner_tests = [\n",
        "    {\"text\": \"Alice and Bob went to the park.\", \"expected\": [\"Alice\", \"Bob\"]},\n",
        "    {\"text\": \"Barack Obama met Joe Biden in Washington.\", \"expected\": [\"Barack Obama\", \"Joe Biden\"]},\n",
        "]\n",
        "\n",
        "def make_ner_prompt(setting, text):\n",
        "    prompt = ner_instruction\n",
        "    if setting == 'one-shot':\n",
        "        prompt += ner_one_shot_example\n",
        "    if setting == 'two-shot':\n",
        "        prompt += ner_one_shot_example + ner_two_shot_example\n",
        "    prompt += f\"Text: {text}\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "def check_ner(answer_text, expected_names):\n",
        "    ans = answer_text.split('Answer:')[-1]\n",
        "    ans_low = ans.lower()\n",
        "    ok = True\n",
        "    for gold in expected_names:\n",
        "        if all(part.lower() in ans_low for part in gold.split()):\n",
        "            continue\n",
        "        else:\n",
        "            ok = False\n",
        "    return ok, ans.strip()\n",
        "\n",
        "math_instruction = (\n",
        "    \"This is a first grade math quiz. Solve the sum. Answer with just the number.\\n\"\n",
        ")\n",
        "math_one_shot_example = (\n",
        "    \"Question: 12 + 7 =\\n\"\n",
        "    \"Answer: 19\\n\"\n",
        ")\n",
        "math_two_shot_example = (\n",
        "    \"Question: 23 + 11 =\\n\"\n",
        "    \"Answer: 34\\n\"\n",
        ")\n",
        "math_tests = [\n",
        "    {\"a\": 11, \"b\": 22},\n",
        "    {\"a\": 47, \"b\": 18},\n",
        "]\n",
        "\n",
        "def make_math_prompt(setting, a, b):\n",
        "    prompt = math_instruction\n",
        "    if setting == 'one-shot':\n",
        "        prompt += math_one_shot_example\n",
        "    if setting == 'two-shot':\n",
        "        prompt += math_one_shot_example + math_two_shot_example\n",
        "    prompt += f\"Question: {a} + {b} =\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "def check_math(answer_text, a, b):\n",
        "    ans = answer_text.split('Answer:')[-1]\n",
        "    m = re.search(r\"[-+]?\\d+\", ans)\n",
        "    if m:\n",
        "        pred = int(m.group(0))\n",
        "    else:\n",
        "        pred = None\n",
        "    correct_value = a + b\n",
        "    return (pred == correct_value), (pred, correct_value, ans.strip())\n",
        "\n",
        "all_results = []\n",
        "settings = ['zero-shot', 'one-shot', 'two-shot']\n",
        "\n",
        "for setting in settings:\n",
        "    # Sentiment\n",
        "    for i, case in enumerate(sentiment_tests, start=1):\n",
        "        prompt = make_sentiment_prompt(setting, case['text'])\n",
        "        gen = generate_en(prompt, max_new_tokens=24)\n",
        "        ok, pred = check_sentiment(gen, case['expected'])\n",
        "        all_results.append(['sentiment', setting, i, case['expected'], pred, ok])\n",
        "\n",
        "    # NER\n",
        "    for i, case in enumerate(ner_tests, start=1):\n",
        "        prompt = make_ner_prompt(setting, case['text'])\n",
        "        gen = generate_en(prompt, max_new_tokens=32)\n",
        "        ok, ans = check_ner(gen, case['expected'])\n",
        "        all_results.append(['name_recognition', setting, i, ', '.join(case['expected']), ans, ok])\n",
        "\n",
        "    # Math\n",
        "    for i, case in enumerate(math_tests, start=1):\n",
        "        prompt = make_math_prompt(setting, case['a'], case['b'])\n",
        "        gen = generate_en(prompt, max_new_tokens=10)\n",
        "        ok, info = check_math(gen, case['a'], case['b'])\n",
        "        pred, gold, raw = info\n",
        "        all_results.append(['two_digit_addition', setting, i, str(gold), str(pred), ok])\n",
        "\n",
        "print(\"Task | Setting | Case | Expected | Predicted | OK\")\n",
        "for row in all_results:\n",
        "    print(\" | \".join([str(x) for x in row]))\n",
        "\n",
        "acc = {}\n",
        "for task, setting, case, exp, pred, ok in all_results:\n",
        "    key = (task, setting)\n",
        "    if key not in acc:\n",
        "        acc[key] = [0,0]\n",
        "    acc[key][1] += 1\n",
        "    if ok:\n",
        "        acc[key][0] += 1\n",
        "\n",
        "print(\"\\nAccuracy by task/setting:\")\n",
        "for key, (good, total) in acc.items():\n",
        "    pct = 100.0 * good / total if total else 0.0\n",
        "    print(f\"{key[0]} / {key[1]}: {good}/{total} = {pct:.1f}%\")\n",
        "\n",
        "print(\"\\nNotes:\")\n",
        "print(\"- GPT-2 is small and not instruction-tuned, so zero-shot can be weak.\")\n",
        "print(\"- One- and two-shot usually help by showing the format.\")\n",
        "print(\"- NER check only verifies that expected names appear in the model's answer.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Submit this exercise by submitting your code and your answers to the above questions as comments on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMPTKW2dgboQJpXpHYIBCHp",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jlab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
